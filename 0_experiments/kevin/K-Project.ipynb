{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import time\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from itertools import combinations\n",
    "from catboost import CatBoostClassifier, cv, Pool \n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_normalize = ['Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', 'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "df_train_norm = df_train.copy()\n",
    "df_train_norm[col_normalize] = normalize(df_train[col_normalize])\n",
    "for col in col_normalize:\n",
    "    print(col)\n",
    "    l = []\n",
    "    for cover in sorted(df_train_norm.Cover_Type.unique()):\n",
    "        l.append( [v for v,k in zip(df_train_norm[col],df_train_norm['Cover_Type']) if k==cover]\n",
    "                )\n",
    "    plt.boxplot(l)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for cover in sorted(df_train_norm.Cover_Type.unique()):\n",
    "    l.append( [math.log(v+1) for v,k in zip(df_train_norm['Horizontal_Distance_To_Roadways'],df_train_norm['Cover_Type']) if k==cover]\n",
    "            )\n",
    "plt.boxplot(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for cover in sorted(df_train_norm.Cover_Type.unique()):\n",
    "    l.append( [v for v,k in zip(df_train_norm['Horizontal_Distance_To_Roadways'],df_train_norm['Cover_Type']) if k==cover]\n",
    "            )\n",
    "plt.boxplot(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = []\n",
    "for k in [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]:\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(df_train_norm.to_numpy(), df_train_norm['Cover_Type'].to_numpy())\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(\"k={}:\\t{}\".format(k,explained_variance))\n",
    "    results.append([k, explained_variance])\n",
    "plt.plot(np.array(results)[:,0], np.array(results)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soils = [x for x in df_train_norm.columns if \"Soil\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in combinations(soils, 3):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train_norm[soils].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "classes = []\n",
    "for n in tqdm(range(2,6)):\n",
    "    for x in tqdm(combinations(soils, n)):\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(df_train_norm[list(x)].to_numpy(), df_train_norm['Cover_Type'].to_numpy())\n",
    "        explained_variance = pca.explained_variance_ratio_.sum()\n",
    "        classes.append([*x] + [\" \"]*(5-n) + [explained_variance])\n",
    "\n",
    "r = pd.DataFrame(classes, columns=['S1','S2','S3','S4','S5','var'])\n",
    "r['var'] -= 1\n",
    "r[r['var']>=0].sort_values('var',ascending=False)\n",
    "\n",
    "r.sort_values('var',ascending=False).to_csv('PCA results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = []\n",
    "for k in [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]:\n",
    "    pca = TruncatedSVD(n_components=k)\n",
    "    pca.fit(df_train_norm.to_numpy(), df_train_norm['Cover_Type'].to_numpy())\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(\"k={}:\\t{}\".format(k,explained_variance))\n",
    "    results.append([k, explained_variance])\n",
    "plt.plot(np.array(results)[:,0], np.array(results)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, version=0):\n",
    "    # 0.8658068783068783\n",
    "    df_n = df.copy()\n",
    "    df_n.drop(columns=['Id'],inplace=True)\n",
    "    df_n = df_n.astype({c:'bool' for c in df_n.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c})\n",
    "    if version >= 1: # 0.893320105820106\n",
    "        col_normalize = ['Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n",
    "                     'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points',\n",
    "                     'Horizontal_Distance_To_Roadways']\n",
    "        df_n['log_Horizontal_Distance_To_Roadways'] = np.log(df_n['Horizontal_Distance_To_Roadways']+1)\n",
    "        df_n['log_Horizontal_Distance_To_Fire_Points'] = np.log(df_n['Horizontal_Distance_To_Fire_Points']+1)\n",
    "        df_n[col_normalize] = normalize(df_n[col_normalize])\n",
    "        df_n.drop(columns=['Soil_Type7'],inplace=True)\n",
    "    if version >= 2: # 0.8964947089947091\n",
    "        df_n['sq_Elevation'] = np.power(df['Elevation'],1.5)\n",
    "        df_n.drop(columns='Aspect',inplace=True)\n",
    "        df_n['norm_aspect'] = df.Aspect.map(lambda x: x-180 if x > 180 else x+180)\n",
    "        df_n['atan_aspect'] = np.arctan(df_n.norm_aspect)\n",
    "    if version >= 3: # 0.9104497354497356\n",
    "        df_n['Vertical_Distance_To_Hydrology'] = np.abs(df_n.Vertical_Distance_To_Hydrology)\n",
    "        df_n['E-VH'] = df.Elevation - df.Vertical_Distance_To_Hydrology * .9 \n",
    "        df_n['E-HH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * .5\n",
    "        \n",
    "        df_n['F+R'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) ** 2\n",
    "        df_n['F+H'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) ** 0.3\n",
    "        df_n['H+R'] = (df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways)\n",
    "        \n",
    "        df_n['abs_H-R'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Roadways)) \n",
    "        df_n['abs_H-F'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Fire_Points)) \n",
    "        df_n['abs_F-R'] = (np.abs(df.Horizontal_Distance_To_Fire_Points - df.Horizontal_Distance_To_Roadways)) \n",
    "    return df_n\n",
    "\n",
    "def submit(model,version):\n",
    "    global df_train\n",
    "    df_train_c = pipeline(df_train.copy(),version)\n",
    "    df_submit = pipeline(df_test.copy(),version)\n",
    "    X, Y = df_train_c.drop(columns=['Cover_Type']).to_numpy(), df_train_c.Cover_Type.to_numpy()\n",
    "    model.fit(X, Y)\n",
    "    pred = model.predict(df_submit.to_numpy())\n",
    "    final_df = df_test.copy()\n",
    "    final_df['Cover_Type'] = pred\n",
    "    return final_df[['Id','Cover_Type']]\n",
    "\n",
    "def score_model(model,df, return_val=False, return_train=False, display=True, return_acc=False, return_time=False, show_weights=False, return_cv_acc=False, verbose=True, single_split=False, return_class_acc=False):\n",
    "    X , Y = df.drop(columns=['Cover_Type']).to_numpy(), df.Cover_Type.to_numpy()\n",
    "    start = time.time()\n",
    "    results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "    if single_split or return_class_acc:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, pred)\n",
    "    end = time.time()\n",
    "    cv_acc = results.mean()\n",
    "    if verbose:\n",
    "        print('cv acc:', cv_acc)\n",
    "        if single_split:\n",
    "            print('split acc:', acc)\n",
    "        print('time taken:', end-start, end='\\n\\n')\n",
    "    if display:\n",
    "        matrix = cm(y_val, pred)\n",
    "        print(matrix.diagonal() / matrix.sum(axis=1))\n",
    "\n",
    "        disp = plot_confusion_matrix(model, X_val, y_val, display_labels=set(y_train), cmap=plt.cm.Blues, normalize='true')\n",
    "        plt.show()\n",
    "    \n",
    "    if show_weights:\n",
    "        for w,k in sorted(list(zip(model.feature_importances_, df.drop(columns=['Cover_Type']).columns)), key=lambda x: x[0]):\n",
    "            print(k,w)\n",
    "            \n",
    "    # return all data\n",
    "    return_data = [model]\n",
    "    if return_train:\n",
    "        return_data += [X_train, y_train]\n",
    "    if return_val:\n",
    "        return_data += [X_val, y_val]\n",
    "    if return_acc:\n",
    "        return_data += [acc]\n",
    "    if return_cv_acc:\n",
    "        return_data += [cv_acc]\n",
    "    if return_time:\n",
    "        return_data += [end-start]\n",
    "    if return_class_acc:\n",
    "        matrix = cm(y_val, pred)\n",
    "        ca = matrix.diagonal() / matrix.sum(axis=1)\n",
    "        return_data += [ca]\n",
    "    return tuple(return_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for clf in [ExtraTreesClassifier(n_jobs=-1, random_state=0),\n",
    "           HistGradientBoostingClassifier(random_state=0),\n",
    "           RandomForestClassifier(n_jobs=-1, random_state=0)]:\n",
    "    \n",
    "    models.append(score_model(clf,df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Ensemble = Mix 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "models = [('et',ExtraTreesClassifier(n_jobs=-1, random_state=0)),\n",
    "           ('hg',HistGradientBoostingClassifier(random_state=0)),\n",
    "           ('rf',RandomForestClassifier(n_jobs=-1, random_state=0))]\n",
    "model = VotingClassifier(models, n_jobs=-1)\n",
    "score_model(model,df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "models = [('hg1',HistGradientBoostingClassifier(random_state=0)),\n",
    "           ('hg2',HistGradientBoostingClassifier(random_state=0)),\n",
    "           ('hg3',HistGradientBoostingClassifier(random_state=0))]\n",
    "model = VotingClassifier(models, n_jobs=-1)\n",
    "score_model(model,df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "models = [('et',ExtraTreesClassifier(n_jobs=-1, random_state=0)),\n",
    "           ('et2',ExtraTreesClassifier(n_jobs=-1, random_state=0)),\n",
    "           ('et3',ExtraTreesClassifier(n_jobs=-1, random_state=0))]\n",
    "model = VotingClassifier(models, n_jobs=-1)\n",
    "score_model(model,df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "models = [('rf1',RandomForestClassifier(n_jobs=-1, random_state=0)),\n",
    "           ('rf2',RandomForestClassifier(n_jobs=-1, random_state=0)),\n",
    "           ('rf3',RandomForestClassifier(n_jobs=-1, random_state=0))]\n",
    "model = VotingClassifier(models, n_jobs=-1)\n",
    "score_model(model,df_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    best_m, best_X_train, best_y_train, best_X_val, best_y_val = score_model(ExtraTreesClassifier(n_jobs=-1, random_state=1189), df_train_norm, return_train = True, return_val = True, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= r\"http://stream1.cmatc.cn/pub/comet/FireWeather/S290Unit10FuelMoisture/comet/fire/s290/unit10/media/graphics/aspsumm.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_jobs=-1, random_state=1189)\n",
    "df_train_norm = pipeline(df_train, 3)\n",
    "_, cv_acc = score_model(model,df_train_norm, display=True, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rs = [x for x in sorted(good_rs, reverse=True)]\n",
    "good_rs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get good Random_state for Voting Model\n",
    "results = []\n",
    "for x in range(6001,7001):\n",
    "    print(x, end='           \\r', flush=True)\n",
    "    model = ExtraTreesClassifier(n_jobs=-1, random_state=x)\n",
    "    df_train_norm = pipeline(df_train, 3)\n",
    "    _, cv_acc = score_model(model,df_train_norm, display=False, return_cv_acc=True, verbose=False);\n",
    "    results.append([cv_acc,x])\n",
    "grs = [r for r in results if r[0] > .91]\n",
    "print(len(grs),' '*50)\n",
    "good_rs = [x for x in sorted(good_rs+grs, reverse=True)]\n",
    "print(len(good_rs),' '*50)\n",
    "print(good_rs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_1 = [[0.9111111111111111, [1189]],\n",
    " [0.9117724867724867, [1189, 883]],\n",
    " [0.9124338624338624, [1189, 883, 2153]],\n",
    " [0.9116402116402116, [1189, 883, 2153, 5568]],\n",
    " [0.9123015873015874, [1189, 883, 2153, 5568, 2077]],\n",
    " [0.9130291005291007, [1189, 883, 2153, 5568, 2077, 769]],\n",
    " [0.9122354497354499, [1189, 883, 2153, 5568, 2077, 769, 2223]],\n",
    " [0.9130952380952382, [1189, 883, 2153, 5568, 2077, 769, 2223, 2675]],\n",
    " [0.9121693121693122, [1189, 883, 2153, 5568, 2077, 769, 2223, 2675, 995]],\n",
    " [0.9119708994708994, [1189, 883, 2153, 5568, 2077, 769, 2223, 2675, 995, 0]],\n",
    " [0.9121693121693122,\n",
    "  [1189, 883, 2153, 5568, 2077, 769, 2223, 2675, 995, 0, 6890]],\n",
    " [0.9123677248677249,\n",
    "  [1189, 883, 2153, 5568, 2077, 769, 2223, 2675, 995, 0, 6890, 1612]],\n",
    " [0.912037037037037,\n",
    "  [1189, 883, 2153, 5568, 2077, 769, 2223, 2675, 995, 0, 6890, 1612, 1548]],\n",
    " [0.9123015873015874,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109]],\n",
    " [0.9119047619047619,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529]],\n",
    " [0.9121031746031745,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389]],\n",
    " [0.9117724867724867,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073]],\n",
    " [0.9121031746031747,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040]],\n",
    " [0.9118386243386244,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915]],\n",
    " [0.9121031746031747,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910]],\n",
    " [0.9119708994708994,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680]],\n",
    " [0.9117063492063492,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680,\n",
    "   4069]],\n",
    " [0.9118386243386242,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680,\n",
    "   4069,\n",
    "   3010]],\n",
    " [0.9115740740740741,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680,\n",
    "   4069,\n",
    "   3010,\n",
    "   1678]],\n",
    " [0.9116402116402117,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680,\n",
    "   4069,\n",
    "   3010,\n",
    "   1678,\n",
    "   1347]],\n",
    " [0.9113756613756614,\n",
    "  [1189,\n",
    "   883,\n",
    "   2153,\n",
    "   5568,\n",
    "   2077,\n",
    "   769,\n",
    "   2223,\n",
    "   2675,\n",
    "   995,\n",
    "   0,\n",
    "   6890,\n",
    "   1612,\n",
    "   1548,\n",
    "   2109,\n",
    "   1529,\n",
    "   389,\n",
    "   3073,\n",
    "   1040,\n",
    "   5915,\n",
    "   5910,\n",
    "   5680,\n",
    "   4069,\n",
    "   3010,\n",
    "   1678,\n",
    "   1347,\n",
    "   729]]]\n",
    "\n",
    "good_rs = [[0.9111111111111111, 1189],\n",
    " [0.9109788359788361, 883],\n",
    " [0.9108465608465609, 2153],\n",
    " [0.9107804232804233, 5568],\n",
    " [0.9106481481481481, 2077],\n",
    " [0.9106481481481481, 769],\n",
    " [0.9105820105820104, 2223],\n",
    " [0.9105158730158729, 2675],\n",
    " [0.9105158730158729, 995],\n",
    " [0.9104497354497356, 0],\n",
    " [0.9104497354497354, 6890],\n",
    " [0.9104497354497354, 1612],\n",
    " [0.9104497354497354, 1548],\n",
    " [0.9103835978835979, 2109],\n",
    " [0.9103835978835979, 1529],\n",
    " [0.9103835978835979, 389],\n",
    " [0.9103835978835978, 3073],\n",
    " [0.9103174603174604, 1040],\n",
    " [0.9103174603174603, 5915],\n",
    " [0.9102513227513228, 5910],\n",
    " [0.9102513227513228, 5680],\n",
    " [0.9102513227513228, 4069],\n",
    " [0.9102513227513228, 3010],\n",
    " [0.9102513227513228, 1678],\n",
    " [0.9102513227513228, 1347],\n",
    " [0.9102513227513228, 729],\n",
    " [0.9102513227513226, 6044],\n",
    " [0.9102513227513226, 1081],\n",
    " [0.9101851851851853, 6893],\n",
    " [0.9101851851851853, 6314],\n",
    " [0.9101851851851853, 6013],\n",
    " [0.9101851851851853, 4022],\n",
    " [0.9101851851851853, 3897],\n",
    " [0.9101851851851853, 3122],\n",
    " [0.9101851851851853, 1633],\n",
    " [0.9101851851851851, 6277],\n",
    " [0.9101851851851851, 4470],\n",
    " [0.9101851851851851, 130],\n",
    " [0.9101190476190476, 5954],\n",
    " [0.9101190476190476, 5207],\n",
    " [0.9101190476190476, 3710],\n",
    " [0.9101190476190476, 2124],\n",
    " [0.9101190476190476, 813],\n",
    " [0.9101190476190476, 50],\n",
    " [0.9100529100529101, 6994],\n",
    " [0.9100529100529101, 6358],\n",
    " [0.9100529100529101, 4229],\n",
    " [0.9100529100529101, 2752],\n",
    " [0.91005291005291, 3281]]\n",
    "\n",
    "et_hp = {0: 1200,\n",
    " 50: 700,\n",
    " 130: 900,\n",
    " 389: 100,\n",
    " 729: 100,\n",
    " 769: 100,\n",
    " 813: 200,\n",
    " 883: 100,\n",
    " 995: 300,\n",
    " 1040: 100,\n",
    " 1081: 1100,\n",
    " 1189: 100,\n",
    " 1347: 100,\n",
    " 1529: 300,\n",
    " 1548: 800,\n",
    " 1612: 100,\n",
    " 1633: 500,\n",
    " 1678: 100,\n",
    " 2077: 100,\n",
    " 2109: 100,\n",
    " 2124: 200,\n",
    " 2153: 100,\n",
    " 2223: 100,\n",
    " 2675: 100,\n",
    " 2752: 200,\n",
    " 3010: 1200,\n",
    " 3073: 100,\n",
    " 3122: 300,\n",
    " 3281: 100,\n",
    " 3710: 100,\n",
    " 3897: 100,\n",
    " 4022: 100,\n",
    " 4069: 1200,\n",
    " 4229: 1100,\n",
    " 4470: 100,\n",
    " 5207: 1200,\n",
    " 5568: 100,\n",
    " 5680: 100,\n",
    " 5910: 100,\n",
    " 5915: 200,\n",
    " 5954: 200,\n",
    " 6013: 1000,\n",
    " 6044: 100,\n",
    " 6277: 300,\n",
    " 6314: 100,\n",
    " 6358: 100,\n",
    " 6890: 100,\n",
    " 6893: 400,\n",
    " 6994: 900}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_1).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('et'+str(y),ExtraTreesClassifier(n_jobs=-1, random_state=y, n_estimators=et_hp[y])) for y in good[:8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ET_voting = [{'name': 'et1189', 'rs': 1189, 'n_tree': 100},\n",
    " {'name': 'et883', 'rs': 883, 'n_tree': 100},\n",
    " {'name': 'et2153', 'rs': 2153, 'n_tree': 100},\n",
    " {'name': 'et5568', 'rs': 5568, 'n_tree': 100},\n",
    " {'name': 'et2077', 'rs': 2077, 'n_tree': 100},\n",
    " {'name': 'et769', 'rs': 769, 'n_tree': 100},\n",
    " {'name': 'et2223', 'rs': 2223, 'n_tree': 100},\n",
    " {'name': 'et2675', 'rs': 2675, 'n_tree': 100}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best voting model\n",
    "# 0.9110449735449736\n",
    "# results_1 = []\n",
    "good = [x[1] for x in good_rs]\n",
    "x = 8\n",
    "models = [('et'+str(y),ExtraTreesClassifier(n_jobs=-1, random_state=y, n_estimators=et_hp[y])) for y in good[:x]]\n",
    "model = VotingClassifier(models, n_jobs=-1)\n",
    "df_train_norm = pipeline(df_train, 3)\n",
    "_, cv_acc = score_model(model,df_train_norm, display=True, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28,13), dpi= 80)\n",
    "plt.scatter(df_train.Horizontal_Distance_To_Fire_Points-df_train.Horizontal_Distance_To_Hydrology,df_train.Horizontal_Distance_To_Hydrology, c=df_train.Cover_Type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for st in soil_types:\n",
    "plt.figure(figsize=(28,13), dpi= 80)\n",
    "sns.violinplot(x='bin_st', y='Cover_Type', data=df_soil, scale='width', inner='box', palette=sns.color_palette(\"pastel\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons to Use:<br>\n",
    "* missing values numeric variables\n",
    "* non-encoded categorical variables\n",
    "* interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.core.display import display, clear_output\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_train\n",
    "df = df_train_dir.copy()\n",
    "df = df.drop(columns=['Id','Cover_Type'])\n",
    "X , Y = df.to_numpy(), df_train.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "\n",
    "cat_features = [df.columns.get_loc(c) for c in df.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c]\n",
    "\n",
    "cv_dataset = Pool(data=X,\n",
    "                  label=Y,\n",
    "                  cat_features=cat_features,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": 1000,\n",
    "          \"loss_function\": \"MultiClassOneVsAll\", # MultiClass\n",
    "          \"verbose\": False}\n",
    "\n",
    "scores = cv(cv_dataset,\n",
    "            params=params,\n",
    "            nfold=10, \n",
    "           )\n",
    "\n",
    "1 - scores[[x for x  in scores.columns if 'mean' in x]].apply(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": 1000,\n",
    "          \"loss_function\": \"MultiClass\", # MultiClass\n",
    "          \"verbose\": False}\n",
    "\n",
    "scores = cv(cv_dataset,\n",
    "            params=params,\n",
    "            nfold=10, \n",
    "           )\n",
    "\n",
    "1 - scores[[x for x  in scores.columns if 'mean' in x]].apply(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = CatBoostClassifier(\n",
    "#                               n_estimators=100,\n",
    "#                               max_depth=10,\n",
    "#                               learning_rate=0.1,\n",
    "                              random_state=0,\n",
    "                              objective='MultiClass', #OneVsAll\n",
    "#                               iterations=100,\n",
    "                              )\n",
    "\n",
    "booster.fit(X_train, y_train, \n",
    "             cat_features = cat_features,\n",
    "             plot=False, verbose=200,\n",
    "             early_stopping_rounds=100,\n",
    "            )\n",
    "\n",
    "test_preds = booster.predict(X_val)\n",
    "train_preds = booster.predict(X_train)\n",
    "\n",
    "print(\"Train Accuracy : %.2f\"%booster.score(X_train, y_train))\n",
    "print(\"\\nTest  Accuracy : %.2f\"%booster.score(X_val, y_val))\n",
    "\n",
    "for w,k in sorted(list(zip(booster.get_feature_importance(), df.columns)), key=lambda x: x[0]):\n",
    "    print(k,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline(df_train, '1')\n",
    "df = df.drop(columns=['Id','Cover_Type'])\n",
    "X , Y = df.to_numpy(), df_train.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "\n",
    "cat_features = [df.columns.get_loc(c) for c in df.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c]\n",
    "\n",
    "cv_dataset = Pool(data=X,\n",
    "                  label=Y,\n",
    "                  cat_features=cat_features,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": 1000,\n",
    "          \"loss_function\": \"MultiClassOneVsAll\", # MultiClass\n",
    "          \"verbose\": False}\n",
    "\n",
    "scores = cv(cv_dataset,\n",
    "            params=params,\n",
    "            nfold=10, \n",
    "           )\n",
    "\n",
    "1 - scores[[x for x  in scores.columns if 'mean' in x]].apply(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": 1000,\n",
    "          \"loss_function\": \"MultiClass\", # MultiClass\n",
    "          \"verbose\": False}\n",
    "\n",
    "scores = cv(cv_dataset,\n",
    "            params=params,\n",
    "            nfold=10, \n",
    "           )\n",
    "\n",
    "1 - scores[[x for x  in scores.columns if 'mean' in x]].apply(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = CatBoostClassifier(\n",
    "#                               n_estimators=100,\n",
    "#                               max_depth=10,\n",
    "#                               learning_rate=0.1,\n",
    "                              random_state=0,\n",
    "                              objective='MultiClass', #OneVsAll\n",
    "#                               iterations=100,\n",
    "                              )\n",
    "\n",
    "booster.fit(X_train, y_train, \n",
    "             cat_features = cat_features,\n",
    "             plot=False, verbose=200,\n",
    "             early_stopping_rounds=100,\n",
    "            )\n",
    "\n",
    "test_preds = booster.predict(X_val)\n",
    "train_preds = booster.predict(X_train)\n",
    "\n",
    "print(\"Train Accuracy : %.2f\"%booster.score(X_train, y_train))\n",
    "print(\"\\nTest  Accuracy : %.2f\"%booster.score(X_val, y_val))\n",
    "\n",
    "for w,k in sorted(list(zip(booster.get_feature_importance(), df.columns)), key=lambda x: x[0]):\n",
    "    print(k,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = CatBoostClassifier(\n",
    "#                               n_estimators=100,\n",
    "#                               max_depth=10,\n",
    "#                               learning_rate=0.1,\n",
    "                              random_state=0,\n",
    "                              objective='MultiClass', #OneVsAll\n",
    "#                               iterations=100,\n",
    "                              )\n",
    "\n",
    "booster.fit(X_train, y_train, \n",
    "             cat_features = cat_features,\n",
    "             plot=False, verbose=200,\n",
    "             early_stopping_rounds=100,\n",
    "            )\n",
    "\n",
    "test_preds = booster.predict(X_val)\n",
    "train_preds = booster.predict(X_train)\n",
    "\n",
    "print(\"Train Accuracy : %.2f\"%booster.score(X_train, y_train))\n",
    "print(\"\\nTest  Accuracy : %.2f\"%booster.score(X_val, y_val))\n",
    "\n",
    "for w,k in sorted(list(zip(booster.get_feature_importance(), df.columns)), key=lambda x: x[0]):\n",
    "    print(k,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = booster.predict(X_val)\n",
    "acc = accuracy_score(y_val, pred)\n",
    "matrix = cm(y_val, pred)\n",
    "print(matrix.diagonal() / matrix.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_estimator(v, rs=1189, model=''):\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    tree_size_acc = []\n",
    "    for n in range(100,1501,100):\n",
    "        print(n, flush=True, end=' '*20+'\\r')\n",
    "        if model == 'ET':\n",
    "            m, acc = score_model(ExtraTreesClassifier(n_estimators=n, n_jobs=-1, random_state=rs), df_train_norm, \n",
    "                                             display=False, return_cv_acc=True, return_time=False, verbose=False)\n",
    "        elif model == 'RF':\n",
    "            m, acc = score_model(RandomForestClassifier(n_estimators=n, n_jobs=-1, random_state=rs), df_train_norm, \n",
    "                                             display=False, return_cv_acc=True, return_time=False, verbose=False)\n",
    "        else:\n",
    "            m, acc = None, None\n",
    "        tree_size_acc.append((n, acc))\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # # ax1 for # time\n",
    "    # l1 = ax.plot(np.array(tree_size_acc)[:,1], np.array(tree_size_acc)[:,2], 'r', label='time')\n",
    "    # ax.tick_params('y', colors='r')\n",
    "    # ax.grid(axis='y', color='lightcoral', linestyle=':')\n",
    "    # # ax2 for # n_estimators\n",
    "    # ax2 = ax.twinx()\n",
    "    # l2 = ax2.plot(np.array(tree_size_acc)[:,1], np.array(tree_size_acc)[:,0], 'b', label='n_estimators')\n",
    "    # ax2.tick_params('y', colors='b')\n",
    "    # ax2.grid(axis='y', color='royalblue', linestyle=(0,(5,5)))\n",
    "    # # general plot\n",
    "    # lines = l1 + l2\n",
    "    # labels = [l.get_label() for l in lines]\n",
    "    # ax.legend(lines,labels)\n",
    "    # plt.show()\n",
    "    return sorted(tree_size_acc, key=lambda x: x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, 3)\n",
    "tree_size_acc = []\n",
    "for n in range(100,1501,100):\n",
    "    print(n, flush=True, end=' '*20+'\\r')\n",
    "    m, acc = score_model(ExtraTreesClassifier(n_estimators=n, n_jobs=-1, random_state=rs), df_train_norm, \n",
    "                                         display=False, return_cv_acc=True, return_time=False, verbose=False)\n",
    "    tree_size_acc.append((n, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# ax for # n_estimators\n",
    "l2 = ax.plot(np.array(tree_size_acc)[:,0], np.array(tree_size_acc)[:,1], label='n_estimators')\n",
    "# general plot\n",
    "ax.legend()\n",
    "ax.set_xlabel('N_Estimators')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.xaxis.label.set_color('w')\n",
    "ax.yaxis.label.set_color('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tree_size_acc, key=lambda x: x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elevation', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
       "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
       "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
       "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
       "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
       "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
       "       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
       "       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15',\n",
       "       'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
       "       'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
       "       'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
       "       'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
       "       'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
       "       'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
       "       'Soil_Type40', 'Cover_Type', 'log_Horizontal_Distance_To_Roadways',\n",
       "       'log_Horizontal_Distance_To_Fire_Points', 'sq_Elevation', 'norm_aspect',\n",
       "       'atan_aspect', 'E-VH', 'E-HH', 'F+R', 'F+H', 'H+R', 'abs_H-R',\n",
       "       'abs_H-F', 'abs_F-R', 'Euclidian_Distance_To_Hydrology'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_norm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 1189\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    model = ExtraTreesClassifier(n_jobs=-1, random_state=rs)\n",
    "    _, cv_acc = score_model(model,df_train_norm, display=False, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ET'\n",
    "rs = 1189\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    best_n = tune_estimator(v, rs, model_name)\n",
    "    model = ExtraTreesClassifier(n_estimators=best_n, random_state=rs, n_jobs=-1)\n",
    "    submit(model, v).to_csv('{}-{}-{}.csv'.format(model_name,v,best_n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['#d62728', '#268C70', \n",
    "     '#2ca02c', '#1f77b4', \n",
    "     '#9467bd', '#608475', \n",
    "     '#e377c2', '#BC6FC0', \n",
    "     '#d62728']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# ax1 for original\n",
    "ax.scatter([1]*len(np.arange(0,361,45)), np.arange(0,361,45), c=c)\n",
    "ax.set_ylim((-20,380))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks(np.arange(0,361,45))\n",
    "ax.set_ylabel('Original Aspect', c='w', size='x-large')\n",
    "ax.set_yticklabels(['N','NE','E','SE','S','SW','W','NW','N'], fontsize='x-large')\n",
    "\n",
    "# ax2 for transformed\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter([2]*len(np.arange(0,361,45)), [x-180 if x > 180 else x+180 for x in np.arange(0,361,45)], c=c)\n",
    "ax2.set_ylim((-20,380))\n",
    "ax2.set_xticks([])\n",
    "ax2.set_ylabel('Transformed Aspect', c='w', size='x-large')\n",
    "ax2.set_yticks(np.arange(45,361,45))\n",
    "ax2.set_yticklabels(['SW','W','NW','N','NE','E','SE','S'], fontsize='x-large')\n",
    "\n",
    "v = .02\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        ax.arrow(1+v, 360-i*45, \n",
    "                 1-v*4.5, -180+10, head_width=0.03, head_length=10, fc='blue', ec='darkblue', ls=':', \n",
    "                 label='Aspect - 180')\n",
    "    else: \n",
    "        ax.arrow(1+v, 360-i*45, \n",
    "             1-v*4.5, -180+10, head_width=0.03, head_length=10, fc='blue', ec='darkblue', ls=':')\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        ax.arrow(1+v, 0+i*45, \n",
    "                 1-v*4.5, 180-10, head_width=0.03, head_length=10, fc='green', ec='darkgreen', ls=(0,(5,5)),\n",
    "                label ='Aspect + 180')\n",
    "    else:\n",
    "        ax.arrow(1+v, 0+i*45, \n",
    "                 1-v*4.5, 180-10, head_width=0.03, head_length=10, fc='green', ec='darkgreen', ls=(0,(5,5)))\n",
    "ax.grid(False)\n",
    "ax2.grid(False)\n",
    "ax.legend(loc='upper center', fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_model = [{'name': 'et1189', 'rs': 1189, 'n_tree': 100},\n",
    "             {'name': 'et883', 'rs': 883, 'n_tree': 100},\n",
    "             {'name': 'et2153', 'rs': 2153, 'n_tree': 100},\n",
    "             {'name': 'et5568', 'rs': 5568, 'n_tree': 100},\n",
    "             {'name': 'et2077', 'rs': 2077, 'n_tree': 100},\n",
    "             {'name': 'et769', 'rs': 769, 'n_tree': 100},\n",
    "             {'name': 'et2223', 'rs': 2223, 'n_tree': 100},\n",
    "             {'name': 'et2675', 'rs': 2675, 'n_tree': 100}]\n",
    "models = [(y['name'],ExtraTreesClassifier(n_jobs=-1, random_state=y['rs'], n_estimators=y['n_tree'])) for y in ets_model]\n",
    "\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    model = VotingClassifier(models, n_jobs=-1)\n",
    "    _, cv_acc = score_model(model,df_train_norm, display=False, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ET_Voting'\n",
    "ets_model = [{'name': 'et1189', 'rs': 1189, 'n_tree': 100},\n",
    "             {'name': 'et883', 'rs': 883, 'n_tree': 100},\n",
    "             {'name': 'et2153', 'rs': 2153, 'n_tree': 100},\n",
    "             {'name': 'et5568', 'rs': 5568, 'n_tree': 100},\n",
    "             {'name': 'et2077', 'rs': 2077, 'n_tree': 100},\n",
    "             {'name': 'et769', 'rs': 769, 'n_tree': 100},\n",
    "             {'name': 'et2223', 'rs': 2223, 'n_tree': 100},\n",
    "             {'name': 'et2675', 'rs': 2675, 'n_tree': 100}]\n",
    "models = [(y['name'],ExtraTreesClassifier(n_jobs=-1, random_state=y['rs'], n_estimators=y['n_tree'])) for y in ets_model]\n",
    "\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    model = VotingClassifier(models, n_jobs=-1)\n",
    "    submit(model, v).to_csv('{}-{}.csv'.format(model_name,v), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 1189\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    model = RandomForestClassifier(n_jobs=-1, random_state=rs)\n",
    "    _, cv_acc = score_model(model,df_train_norm, display=False, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'\n",
    "rs = 1189\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    best_n = tune_estimator(v, rs, model_name)\n",
    "    model = RandomForestClassifier(n_estimators=best_n, random_state=rs, n_jobs=-1)\n",
    "    submit(model, v).to_csv('{}-{}-{}.csv'.format(model_name,v,best_n), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    cat_features = [df_train_norm.columns.get_loc(c) for c in df_train_norm.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c]\n",
    "    model = CatBoostClassifier(\n",
    "                cat_features=cat_features,\n",
    "                random_state=None,\n",
    "                objective='MultiClass',\n",
    "                verbose=False\n",
    "                )\n",
    "    _, cv_acc = score_model(model,df_train_norm, display=False, return_cv_acc=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CatBoost'\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    cat_features = [df_train_norm.columns.get_loc(c) for c in df_train_norm.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c]\n",
    "    model = CatBoostClassifier(\n",
    "                cat_features=cat_features,\n",
    "                objective='MultiClass',\n",
    "                verbose=False\n",
    "                )\n",
    "    submit(model, v).to_csv('{}-{}.csv'.format(model_name,v), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cycler\n",
    "cs = ['#EE6666', '#7abf0a', '#9988DD',\n",
    "      '#EECC55', '#88BB44', '#3a32d1',\n",
    "      '#391306', '#3388BB', '#1DC690']\n",
    "colors = cycler('color', cs)\n",
    "plt.rc('axes', facecolor='#E6E6E6', edgecolor='none',\n",
    "       axisbelow=True, grid=True, prop_cycle=colors)\n",
    "plt.rc('grid', color='w', linestyle='solid')\n",
    "plt.rc('xtick', direction='out', color='w')\n",
    "plt.rc('ytick', direction='out', color='w')\n",
    "plt.rc('patch', edgecolor='#E6E6E6')\n",
    "plt.rc('lines', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline version acc change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_acc = []\n",
    "rs = 1189\n",
    "for v in range(0,4):\n",
    "    print(v)\n",
    "    df_train_norm = pipeline(df_train, v)\n",
    "    model = ExtraTreesClassifier(n_jobs=-1, random_state=rs)\n",
    "    _, class_acc = score_model(model,df_train_norm, display=False, return_cv_acc=False, verbose=True, return_class_acc=True);\n",
    "    pv_acc.append((v,*class_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df = pd.DataFrame(pv_acc)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "for x in range(1,pv_df.shape[1]):\n",
    "    ax.plot(pv_df[0], pv_df[x] - pv_df[x].min(), label='Cover Type {}'.format(x))\n",
    "    \n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xlabel('Feature Version')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.xaxis.label.set_color('limegreen')\n",
    "ax.xaxis.label.set_size('large')\n",
    "ax.yaxis.label.set_color('limegreen')\n",
    "ax.yaxis.label.set_size('large')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_excel('model_results.xlsx')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "\n",
    "# cmap = plt.cm.get_cmap('copper',len(result_df.model.unique()))\n",
    "for i, model_name in enumerate(result_df.model.unique()):\n",
    "    df_r = result_df[result_df.model == model_name]\n",
    "    ax.plot(df_r.version, df_r.acc, label='{} training CV accuracy'.format(model_name), c=cs[i])\n",
    "    ax.plot(df_r.version, df_r.k_acc, linestyle='--', dashes=(5,5) , label='{} test accuracy'.format(model_name), c=cs[i])\n",
    "    \n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xlabel('Feature Version')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.xaxis.label.set_color('limegreen')\n",
    "ax.xaxis.label.set_size('large')\n",
    "ax.yaxis.label.set_color('limegreen')\n",
    "ax.yaxis.label.set_size('large')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "\n",
    "for i, model_name in enumerate(result_df.model.unique()):\n",
    "    df_r = result_df[result_df.model == model_name]\n",
    "    ax.plot(df_r.version, df_r.time, label='{} - time taken'.format(model_name), c=cs[i])\n",
    "    \n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xlabel('Feature Version')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.xaxis.label.set_color('limegreen')\n",
    "ax.xaxis.label.set_size('large')\n",
    "ax.yaxis.label.set_color('limegreen')\n",
    "ax.yaxis.label.set_size('large')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
