{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# normalization\n",
    "col_normalize = ['Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', 'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "df_train_norm = df_train.copy()\n",
    "df_train_norm['Distance_To_Hydrology'] = (df_train_norm['Horizontal_Distance_To_Hydrology']**2 + df_train_norm['Vertical_Distance_To_Hydrology']**2)**(1/2)\n",
    "df_train_norm[col_normalize] = normalize(df_train[col_normalize])\n",
    "df_train_norm['log_Horizontal_Distance_To_Roadways'] = (df_train['Horizontal_Distance_To_Roadways']+1).apply(np.log)\n",
    "df_train_norm['log_Elevation'] = ((df_train_norm['Elevation']**1.5)+1).apply(np.log)\n",
    "df_train_norm.drop(columns=['Elevation','Horizontal_Distance_To_Roadways'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "np.set_printoptions(precision=5)\n",
    "def score_model(model,df, return_val=False, return_train=False, display=True, return_acc=False, return_time=False, show_weights=False):\n",
    "    X , Y = df.drop(columns=['Id','Cover_Type']).to_numpy(), df.Cover_Type.to_numpy()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "    start = time.time()\n",
    "    results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    end = time.time()\n",
    "    print('\\nModel:',type(model).__name__)\n",
    "    print('\\tcv acc:', round(results.mean(),4))\n",
    "    print('\\tsplit acc:', round(acc,4))\n",
    "    print('\\ttime taken:', round(end-start, 4))\n",
    "    if display:\n",
    "        matrix = cm(y_val, pred)\n",
    "        print('\\t', matrix.diagonal() / matrix.sum(axis=1))\n",
    "\n",
    "        disp = plot_confusion_matrix(model, X_val, y_val, display_labels=set(y_train), cmap=plt.cm.Blues, normalize='true')\n",
    "        plt.show()\n",
    "    \n",
    "    if show_weights:\n",
    "        for w,k in sorted(list(zip(model.feature_importances_, df.drop(columns=['Id','Cover_Type']).columns)), key=lambda x: x[0]):\n",
    "            print(k,w)\n",
    "            \n",
    "    # return all data\n",
    "    return_data = [model]\n",
    "    if return_train:\n",
    "        return_data += [X_train, y_train]\n",
    "    if return_val:\n",
    "        return_data += [X_val, y_val]\n",
    "    if return_acc:\n",
    "        return_data += [acc]\n",
    "    if return_time:\n",
    "        return_data += [end-start]\n",
    "    return tuple(return_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dataset for EDA manipulation\n",
    "df_eda = df_train_norm.copy()\n",
    "\n",
    "# This creates 2 new columns that summarize the Wilderness Area and Soil Type columns for ease of visualization\n",
    "df_eda['Wilderness_Area'] = df_eda.iloc[:,11:15].idxmax(axis=1).str.replace('Wilderness_Area','')\n",
    "df_eda['Soil_Type'] = df_eda.iloc[:,16:55].idxmax(axis=1).str.replace('Soil_Type','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['Wilderness_Areas'] = df_eda['Wilderness_Area1'] + df_eda['Wilderness_Area2'] + df_eda['Wilderness_Area3'] + df_eda['Wilderness_Area4']\n",
    "\n",
    "plt.figure()\n",
    "df_one = df_eda[df_eda['Cover_Type'] == 1]\n",
    "sn.histplot(data=df_one, x='Elevation')\n",
    "\n",
    "plt.figure()\n",
    "df_two = df_eda[df_eda['Cover_Type'] == 2]\n",
    "sn.histplot(data=df_two, x='Elevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 2) | (df_train_norm['Cover_Type'] == 1)]\n",
    "X , Y = df_train_sp.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j) + \". \" + df_train_sp.columns[ind_sc[i] + 1] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kevin's code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, version=0):\n",
    "    df_n = df.copy()\n",
    "    df_n.drop(columns=['Id'],inplace=True)\n",
    "    df_n = df_n.astype({c:'bool' for c in df_n.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c})\n",
    "    if version >= 1: \n",
    "        col_normalize = ['Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n",
    "                     'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points',\n",
    "                     'Horizontal_Distance_To_Roadways']\n",
    "        df_n['log_Horizontal_Distance_To_Roadways'] = np.log(df_n['Horizontal_Distance_To_Roadways']+1)\n",
    "        df_n['log_Horizontal_Distance_To_Fire_Points'] = np.log(df_n['Horizontal_Distance_To_Fire_Points']+1)\n",
    "        df_n[col_normalize] = normalize(df_n[col_normalize])\n",
    "        df_n.drop(columns=['Soil_Type7'],inplace=True)\n",
    "    if version >= 2: # 0.8964947089947091\n",
    "        df_n['sq_Elevation'] = np.power(df['Elevation'],1.5)\n",
    "        df_n.drop(columns='Aspect',inplace=True)\n",
    "        df_n['norm_aspect'] = df.Aspect.map(lambda x: x-180 if x > 180 else x+180) # np.abs(df.Aspect - 180)\n",
    "    if version >= 3: # 0.9104497354497356\n",
    "        df_n['Vertical_Distance_To_Hydrology'] = np.abs(df_n.Vertical_Distance_To_Hydrology)\n",
    "        df_n['E-VH'] = df.Elevation - df.Vertical_Distance_To_Hydrology * .9 \n",
    "        df_n['E-HH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * .5\n",
    "        \n",
    "        \n",
    "        df_n['F+R'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) ** 2\n",
    "        df_n['F+H'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) ** 0.3\n",
    "        df_n['H+R'] = (df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways)\n",
    "        \n",
    "        df_n['abs_H-R'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Roadways)) \n",
    "        df_n['abs_H-F'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Fire_Points)) \n",
    "        df_n['abs_F-R'] = (np.abs(df.Horizontal_Distance_To_Fire_Points - df.Horizontal_Distance_To_Roadways)) \n",
    "    return df_n\n",
    "\n",
    "def soils(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    soils = ['Soil_Type' + str(i) for i in range(1,41) if i != 7]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Soil_Type1')\n",
    "    fort_ind = list(df_train_norm_copy.columns).index('Soil_Type40')\n",
    "    df_train_norm_copy['Soil_Type'] = (df_train_norm_copy.iloc[:, one_ind:fort_ind] == 1).idxmax(1).str.replace('Soil_Type','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=soils, inplace=True)\n",
    "    return df_train_norm_copy\n",
    "\n",
    "def wa(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    was = ['Wilderness_Area' + str(i) for i in range(1,5)]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Wilderness_Area1')\n",
    "    four_ind = list(df_train_norm_copy.columns).index('Wilderness_Area4')\n",
    "    df_train_norm_copy['Wilderness_Area'] = (df_train_norm_copy.iloc[:, one_ind:four_ind] == 1).idxmax(1).str.replace('Wilderness_Area','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=was, inplace=True)\n",
    "    return df_train_norm_copy\n",
    "\n",
    "def submit(model,version):\n",
    "    global df_train\n",
    "    df_train_c = pipeline(df_train.copy(),version)\n",
    "    df_submit = pipeline(df_test.copy(),version)\n",
    "    X, Y = df_train_c.drop(columns=['Cover_Type']).to_numpy(), df_train_c.Cover_Type.to_numpy()\n",
    "    model.fit(X, Y)\n",
    "    pred = model.predict(df_submit.to_numpy())\n",
    "    final_df = df_test.copy()\n",
    "    final_df['Cover_Type'] = pred\n",
    "    return final_df[['Id','Cover_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "df_train_norm_1_2 = df_train_norm[(df_train_norm['Cover_Type'] == 1) | (df_train_norm['Cover_Type'] == 2)]\n",
    "\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "X , Y = df_train_norm_1_2.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_norm_1_2.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "predict_fn = lambda x: etc.predict_proba(x).astype(float)\n",
    "df_train_exp=df_train_norm.drop(columns=['Id','Cover_Type'])\n",
    "# Create Lime Explainer\n",
    "explainer =  lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = list(df_train_exp.columns), class_names = ['1','2'])\n",
    "\n",
    "ones = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 1]\n",
    "twos = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 2]\n",
    "\n",
    "\n",
    "k = 0\n",
    "examples_one = []\n",
    "examples_two = []\n",
    "\n",
    "for i in range(len(ones)):\n",
    "    exp = explainer.explain_instance(X_val[ones[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[ones[i]]])[0])\n",
    "    if probs.argmax() != 1:\n",
    "        continue\n",
    "    \n",
    "    k+=1\n",
    "    examples_one.append(X_val[ones[i]])\n",
    "    if k == 75:\n",
    "        break\n",
    "\n",
    "j = 0\n",
    "for i in range(len(twos)):\n",
    "    exp = explainer.explain_instance(X_val[twos[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[twos[i]]])[0])\n",
    "    if probs.argmax() != 0:\n",
    "        continue\n",
    "    \n",
    "    j+=1\n",
    "    examples_two.append(X_val[twos[i]])\n",
    "    if j == 75:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_one\n",
    "\n",
    "df_one = pd.DataFrame(np.row_stack(examples_one), columns=list(df_train_exp.columns))\n",
    "df_two = pd.DataFrame(np.row_stack(examples_two), columns=list(df_train_exp.columns))\n",
    "\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_one.columns:\n",
    "    plt.figure()\n",
    "    print(i)\n",
    "    df_one[i].astype('float').hist()\n",
    "    plt.xlabel(i)\n",
    "    plt.figure()\n",
    "    df_two[i].astype('float').hist()\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['Elevation'].astype('float').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_one.mean())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_one.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_two.mean())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_two.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "# df_train_norm = soils(df_train_norm)\n",
    "# df_train_norm = wa(df_train_norm)\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "score_model(etc, df_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "sn.histplot(data=df_train_norm, x='Vertical_Distance_To_Hydrology')\n",
    "plt.figure()\n",
    "sn.histplot(data=df_train_norm, x='norm_aspect')\n",
    "plt.figure()\n",
    "sn.histplot(data=df_train, x='Aspect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "df_train_norm_1_2 = df_train_norm[(df_train_norm['Cover_Type'] == 3) | (df_train_norm['Cover_Type'] == 6)]\n",
    "\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "X , Y = df_train_norm_1_2.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_norm_1_2.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "predict_fn = lambda x: etc.predict_proba(x).astype(float)\n",
    "df_train_exp=df_train_norm_1_2.drop(columns=['Id','Cover_Type'])\n",
    "# Create Lime Explainer\n",
    "explainer =  lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = list(df_train_exp.columns), class_names = ['3','6'])\n",
    "\n",
    "ones = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 3]\n",
    "k=0\n",
    "for i in range(len(ones)):\n",
    "    exp = explainer.explain_instance(X_val[ones[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[ones[i]]])[0])\n",
    "    if probs.argmax() != 1:\n",
    "        continue\n",
    "    print(probs)\n",
    "    k+=1\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    if k == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df_one = df_train_norm[df_train_norm['Cover_Type'] == 3]\n",
    "\n",
    "sn.histplot(data=df_one, x='sl_asp')\n",
    "\n",
    "plt.figure()\n",
    "df_two = df_train_norm[df_train_norm['Cover_Type'] == 6]\n",
    "sn.histplot(data=df_two, x='sl_asp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "# if not installed ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "n_estimator = [100, 200, 500]\n",
    "boosting_types = ['gbdt', 'dart', 'goss']\n",
    "max_depth = [4, 6, 8]\n",
    "num_leaves = [70, 90, 100]\n",
    "output = {}\n",
    "df_train_c = pipeline(df_train, version=3)\n",
    "\n",
    "for i in n_estimator:\n",
    "    for k in boosting_types:\n",
    "        for l in max_depth:\n",
    "            for q in num_leaves:\n",
    "                model = lgb.LGBMClassifier(n_estimators=i, boosting_type=k, max_depth=l, num_leaves=q)\n",
    "                start = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                acc = accuracy_score(y_val, pred)\n",
    "                end = time.time()\n",
    "                output[str(k) + '_' + str(i) + '_' + str(l) + '_' + str(q)] = [acc, end-start]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperparameters: boosting_Type: 'gbdt', n_estimators = 100, max_depth = 8, num_leaves = 90\")\n",
    "print(\"With the best hyperparameters, extra tree model achieves: \" + \" 0.8806 accuracy in 2.85 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pipeline(df_test.copy(),version=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
