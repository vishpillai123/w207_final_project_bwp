{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "      <th>Distance_To_Hydrology</th>\n",
       "      <th>log_Horizontal_Distance_To_Roadways</th>\n",
       "      <th>log_Elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.040989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035111</td>\n",
       "      <td>0.036858</td>\n",
       "      <td>0.023513</td>\n",
       "      <td>0.997552</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>6.236370</td>\n",
       "      <td>11.792598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0.035262</td>\n",
       "      <td>0.037666</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.997755</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>212.084889</td>\n",
       "      <td>5.968708</td>\n",
       "      <td>11.789127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>0.038115</td>\n",
       "      <td>0.038766</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.997010</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>275.769832</td>\n",
       "      <td>8.064951</td>\n",
       "      <td>11.908210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>0.038208</td>\n",
       "      <td>0.038208</td>\n",
       "      <td>0.019586</td>\n",
       "      <td>0.997096</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>269.235956</td>\n",
       "      <td>8.036250</td>\n",
       "      <td>11.898012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.024740</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.035574</td>\n",
       "      <td>0.037838</td>\n",
       "      <td>0.024255</td>\n",
       "      <td>0.998023</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>153.003268</td>\n",
       "      <td>5.971262</td>\n",
       "      <td>11.792020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15115</th>\n",
       "      <td>15116</td>\n",
       "      <td>0.175948</td>\n",
       "      <td>0.016654</td>\n",
       "      <td>0.186809</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.123091</td>\n",
       "      <td>0.181740</td>\n",
       "      <td>0.154950</td>\n",
       "      <td>0.928252</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>258.094944</td>\n",
       "      <td>6.493754</td>\n",
       "      <td>11.798941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15116</th>\n",
       "      <td>15117</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.414829</td>\n",
       "      <td>0.127791</td>\n",
       "      <td>0.163179</td>\n",
       "      <td>0.144830</td>\n",
       "      <td>0.059636</td>\n",
       "      <td>0.868323</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>662.354890</td>\n",
       "      <td>6.428105</td>\n",
       "      <td>11.796637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15117</th>\n",
       "      <td>15118</td>\n",
       "      <td>0.103008</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>0.280582</td>\n",
       "      <td>0.089940</td>\n",
       "      <td>0.192180</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>0.063804</td>\n",
       "      <td>0.912469</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>383.293621</td>\n",
       "      <td>5.817111</td>\n",
       "      <td>11.731269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15118</th>\n",
       "      <td>15119</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.209985</td>\n",
       "      <td>0.097287</td>\n",
       "      <td>0.220581</td>\n",
       "      <td>0.228287</td>\n",
       "      <td>0.114625</td>\n",
       "      <td>0.897735</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>240.260276</td>\n",
       "      <td>5.493061</td>\n",
       "      <td>11.728257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15119</th>\n",
       "      <td>15120</td>\n",
       "      <td>0.187376</td>\n",
       "      <td>0.032339</td>\n",
       "      <td>0.303416</td>\n",
       "      <td>0.074190</td>\n",
       "      <td>0.179767</td>\n",
       "      <td>0.232080</td>\n",
       "      <td>0.155988</td>\n",
       "      <td>0.869350</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>328.397625</td>\n",
       "      <td>5.602119</td>\n",
       "      <td>11.721002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15120 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0          1  0.008102  0.000477                          0.040989   \n",
       "1          2  0.008976  0.000321                          0.033980   \n",
       "2          3  0.022641  0.001466                          0.043653   \n",
       "3          4  0.024883  0.002890                          0.038850   \n",
       "4          5  0.007277  0.000323                          0.024740   \n",
       "...      ...       ...       ...                               ...   \n",
       "15115  15116  0.175948  0.016654                          0.186809   \n",
       "15116  15117  0.079296  0.012451                          0.414829   \n",
       "15117  15118  0.103008  0.019218                          0.280582   \n",
       "15118  15119  0.160860  0.026971                          0.209985   \n",
       "15119  15120  0.187376  0.032339                          0.303416   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Hillshade_9am  Hillshade_Noon  \\\n",
       "0                            0.000000       0.035111        0.036858   \n",
       "1                           -0.000962       0.035262        0.037666   \n",
       "2                            0.010587       0.038115        0.038766   \n",
       "3                            0.018943       0.038208        0.038208   \n",
       "4                           -0.000162       0.035574        0.037838   \n",
       "...                               ...            ...             ...   \n",
       "15115                        0.005068       0.123091        0.181740   \n",
       "15116                        0.127791       0.163179        0.144830   \n",
       "15117                        0.089940       0.192180        0.169118   \n",
       "15118                        0.097287       0.220581        0.228287   \n",
       "15119                        0.074190       0.179767        0.232080   \n",
       "\n",
       "       Hillshade_3pm  Horizontal_Distance_To_Fire_Points  Wilderness_Area1  \\\n",
       "0           0.023513                            0.997552                 1   \n",
       "1           0.024203                            0.997755                 1   \n",
       "2           0.021989                            0.997010                 1   \n",
       "3           0.019586                            0.997096                 1   \n",
       "4           0.024255                            0.998023                 1   \n",
       "...              ...                                 ...               ...   \n",
       "15115       0.154950                            0.928252                 0   \n",
       "15116       0.059636                            0.868323                 0   \n",
       "15117       0.063804                            0.912469                 0   \n",
       "15118       0.114625                            0.897735                 0   \n",
       "15119       0.155988                            0.869350                 0   \n",
       "\n",
       "       ...  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  Soil_Type39  \\\n",
       "0      ...            0            0            0            0            0   \n",
       "1      ...            0            0            0            0            0   \n",
       "2      ...            0            0            0            0            0   \n",
       "3      ...            0            0            0            0            0   \n",
       "4      ...            0            0            0            0            0   \n",
       "...    ...          ...          ...          ...          ...          ...   \n",
       "15115  ...            0            0            0            0            0   \n",
       "15116  ...            0            0            0            0            0   \n",
       "15117  ...            0            0            0            0            0   \n",
       "15118  ...            0            0            0            0            0   \n",
       "15119  ...            0            0            0            0            0   \n",
       "\n",
       "       Soil_Type40  Cover_Type  Distance_To_Hydrology  \\\n",
       "0                0           5             258.000000   \n",
       "1                0           5             212.084889   \n",
       "2                0           2             275.769832   \n",
       "3                0           2             269.235956   \n",
       "4                0           5             153.003268   \n",
       "...            ...         ...                    ...   \n",
       "15115            0           3             258.094944   \n",
       "15116            0           3             662.354890   \n",
       "15117            0           3             383.293621   \n",
       "15118            0           3             240.260276   \n",
       "15119            0           3             328.397625   \n",
       "\n",
       "       log_Horizontal_Distance_To_Roadways  log_Elevation  \n",
       "0                                 6.236370      11.792598  \n",
       "1                                 5.968708      11.789127  \n",
       "2                                 8.064951      11.908210  \n",
       "3                                 8.036250      11.898012  \n",
       "4                                 5.971262      11.792020  \n",
       "...                                    ...            ...  \n",
       "15115                             6.493754      11.798941  \n",
       "15116                             6.428105      11.796637  \n",
       "15117                             5.817111      11.731269  \n",
       "15118                             5.493061      11.728257  \n",
       "15119                             5.602119      11.721002  \n",
       "\n",
       "[15120 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# normalization\n",
    "col_normalize = ['Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', 'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "df_train_norm = df_train.copy()\n",
    "df_train_norm['Distance_To_Hydrology'] = (df_train_norm['Horizontal_Distance_To_Hydrology']**2 + df_train_norm['Vertical_Distance_To_Hydrology']**2)**(1/2)\n",
    "df_train_norm[col_normalize] = normalize(df_train[col_normalize])\n",
    "df_train_norm['log_Horizontal_Distance_To_Roadways'] = (df_train['Horizontal_Distance_To_Roadways']+1).apply(np.log)\n",
    "df_train_norm['log_Elevation'] = ((df_train_norm['Elevation']**1.5)+1).apply(np.log)\n",
    "df_train_norm.drop(columns=['Elevation','Horizontal_Distance_To_Roadways'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "np.set_printoptions(precision=5)\n",
    "def score_model(model,df, return_val=False, return_train=False, display=True, return_acc=False, return_time=False, show_weights=False):\n",
    "    X , Y = df.drop(columns=['Id','Cover_Type']).to_numpy(), df.Cover_Type.to_numpy()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "    start = time.time()\n",
    "    results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    end = time.time()\n",
    "    print('\\nModel:',type(model).__name__)\n",
    "    print('\\tcv acc:', round(results.mean(),4))\n",
    "    print('\\tsplit acc:', round(acc,4))\n",
    "    print('\\ttime taken:', round(end-start, 4))\n",
    "    if display:\n",
    "        matrix = cm(y_val, pred)\n",
    "        print('\\t', matrix.diagonal() / matrix.sum(axis=1))\n",
    "\n",
    "        disp = plot_confusion_matrix(model, X_val, y_val, display_labels=set(y_train), cmap=plt.cm.Blues, normalize='true')\n",
    "        plt.show()\n",
    "    \n",
    "    if show_weights:\n",
    "        for w,k in sorted(list(zip(model.feature_importances_, df.drop(columns=['Id','Cover_Type']).columns)), key=lambda x: x[0]):\n",
    "            print(k,w)\n",
    "            \n",
    "    # return all data\n",
    "    return_data = [model]\n",
    "    if return_train:\n",
    "        return_data += [X_train, y_train]\n",
    "    if return_val:\n",
    "        return_data += [X_val, y_val]\n",
    "    if return_acc:\n",
    "        return_data += [acc]\n",
    "    if return_time:\n",
    "        return_data += [end-start]\n",
    "    return tuple(return_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dataset for EDA manipulation\n",
    "df_eda = df_train_norm.copy()\n",
    "\n",
    "# This creates 2 new columns that summarize the Wilderness Area and Soil Type columns for ease of visualization\n",
    "df_eda['Wilderness_Area'] = df_eda.iloc[:,11:15].idxmax(axis=1).str.replace('Wilderness_Area','')\n",
    "df_eda['Soil_Type'] = df_eda.iloc[:,16:55].idxmax(axis=1).str.replace('Soil_Type','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda['Wilderness_Areas'] = df_eda['Wilderness_Area1'] + df_eda['Wilderness_Area2'] + df_eda['Wilderness_Area3'] + df_eda['Wilderness_Area4']\n",
    "\n",
    "plt.figure()\n",
    "df_one = df_eda[df_eda['Cover_Type'] == 1]\n",
    "sn.histplot(data=df_one, x='Elevation')\n",
    "\n",
    "plt.figure()\n",
    "df_two = df_eda[df_eda['Cover_Type'] == 2]\n",
    "sn.histplot(data=df_two, x='Elevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 2) | (df_train_norm['Cover_Type'] == 1)]\n",
    "X , Y = df_train_sp.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j) + \". \" + df_train_sp.columns[ind_sc[i] + 1] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kevin's code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df, version=0):\n",
    "    df_n = df.copy()\n",
    "    df_n = df_n.astype({c:'bool' for c in df_n.columns if \"Soil_Type\" in c or \"Wilderness_Area\" in c})\n",
    "    if version >= 1: \n",
    "        col_normalize = ['Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n",
    "                     'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points',\n",
    "                     'Horizontal_Distance_To_Roadways']\n",
    "        df_n['log_Horizontal_Distance_To_Roadways'] = np.log(df_n['Horizontal_Distance_To_Roadways']+1)\n",
    "        df_n['log_Horizontal_Distance_To_Fire_Points'] = np.log(df_n['Horizontal_Distance_To_Fire_Points']+1)\n",
    "        df_n[col_normalize] = normalize(df_n[col_normalize])\n",
    "        df_n.drop(columns=['Soil_Type7'],inplace=True)\n",
    "    if version >= 2: # 0.8964947089947091\n",
    "        df_n['sq_Elevation'] = np.power(df['Elevation'],1.5)\n",
    "        df_n.drop(columns='Aspect',inplace=True)\n",
    "        df_n['norm_aspect'] = df.Aspect.map(lambda x: x-180 if x > 180 else x+180) # np.abs(df.Aspect - 180)\n",
    "    if version >= 3: # 0.9104497354497356\n",
    "        df_n['Vertical_Distance_To_Hydrology'] = np.abs(df_n.Vertical_Distance_To_Hydrology)\n",
    "        df_n['E-VH'] = df.Elevation - df.Vertical_Distance_To_Hydrology * .9 \n",
    "        df_n['E-HH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * .5\n",
    "        \n",
    "        \n",
    "        df_n['F+R'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) ** 2\n",
    "        df_n['F+H'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) ** 0.3\n",
    "        df_n['H+R'] = (df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways)\n",
    "        \n",
    "        df_n['abs_H-R'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Roadways)) \n",
    "        df_n['abs_H-F'] = (np.abs(df.Horizontal_Distance_To_Hydrology - df.Horizontal_Distance_To_Fire_Points)) \n",
    "        df_n['abs_F-R'] = (np.abs(df.Horizontal_Distance_To_Fire_Points - df.Horizontal_Distance_To_Roadways)) \n",
    "    return df_n\n",
    "\n",
    "def soils(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    soils = ['Soil_Type' + str(i) for i in range(1,41) if i != 7]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Soil_Type1')\n",
    "    fort_ind = list(df_train_norm_copy.columns).index('Soil_Type40')\n",
    "    df_train_norm_copy['Soil_Type'] = (df_train_norm_copy.iloc[:, one_ind:fort_ind] == 1).idxmax(1).str.replace('Soil_Type','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=soils, inplace=True)\n",
    "    return df_train_norm_copy\n",
    "\n",
    "def wa(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    was = ['Wilderness_Area' + str(i) for i in range(1,5)]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Wilderness_Area1')\n",
    "    four_ind = list(df_train_norm_copy.columns).index('Wilderness_Area4')\n",
    "    df_train_norm_copy['Wilderness_Area'] = (df_train_norm_copy.iloc[:, one_ind:four_ind] == 1).idxmax(1).str.replace('Wilderness_Area','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=was, inplace=True)\n",
    "    return df_train_norm_copy\n",
    "\n",
    "def submit(model,version):\n",
    "    global df_train\n",
    "    df_train_c = pipeline(df_train.copy(),version)\n",
    "    df_submit = pipeline(df_test.copy(),version)\n",
    "    X, Y = df_train_c.drop(columns=['Cover_Type']).to_numpy(), df_train_c.Cover_Type.to_numpy()\n",
    "    model.fit(X, Y)\n",
    "    pred = model.predict(df_submit.to_numpy())\n",
    "    final_df = df_test.copy()\n",
    "    final_df['Cover_Type'] = pred\n",
    "    return final_df[['Id','Cover_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "df_train_norm_1_2 = df_train_norm[(df_train_norm['Cover_Type'] == 1) | (df_train_norm['Cover_Type'] == 2)]\n",
    "\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "X , Y = df_train_norm_1_2.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_norm_1_2.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "predict_fn = lambda x: etc.predict_proba(x).astype(float)\n",
    "df_train_exp=df_train_norm.drop(columns=['Id','Cover_Type'])\n",
    "# Create Lime Explainer\n",
    "explainer =  lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = list(df_train_exp.columns), class_names = ['1','2'])\n",
    "\n",
    "ones = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 1]\n",
    "twos = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 2]\n",
    "\n",
    "\n",
    "k = 0\n",
    "examples_one = []\n",
    "examples_two = []\n",
    "\n",
    "for i in range(len(ones)):\n",
    "    exp = explainer.explain_instance(X_val[ones[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[ones[i]]])[0])\n",
    "    if probs.argmax() != 1:\n",
    "        continue\n",
    "    \n",
    "    k+=1\n",
    "    examples_one.append(X_val[ones[i]])\n",
    "    if k == 25:\n",
    "        break\n",
    "\n",
    "j = 0\n",
    "for i in range(len(twos)):\n",
    "    exp = explainer.explain_instance(X_val[twos[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[twos[i]]])[0])\n",
    "    if probs.argmax() != 0:\n",
    "        continue\n",
    "    \n",
    "    j+=1\n",
    "    examples_two.append(X_val[twos[i]])\n",
    "    if j == 25:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_one\n",
    "\n",
    "df_one = pd.DataFrame(np.row_stack(examples_one), columns=list(df_train_exp.columns))\n",
    "df_two = pd.DataFrame(np.row_stack(examples_two), columns=list(df_train_exp.columns))\n",
    "\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_one.columns:\n",
    "    plt.figure()\n",
    "    df_one[i].astype('float').hist()\n",
    "    plt.xlabel(i)\n",
    "    plt.figure()\n",
    "    df_two[i].astype('float').hist()\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['Elevation'].astype('float').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_one.mean())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_one.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_two.mean())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df_two.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "# df_train_norm = soils(df_train_norm)\n",
    "# df_train_norm = wa(df_train_norm)\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "score_model(etc, df_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "sn.histplot(data=df_train_norm, x='Vertical_Distance_To_Hydrology')\n",
    "plt.figure()\n",
    "sn.histplot(data=df_train_norm, x='norm_aspect')\n",
    "plt.figure()\n",
    "sn.histplot(data=df_train, x='Aspect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "df_train_norm_1_2 = df_train_norm[(df_train_norm['Cover_Type'] == 3) | (df_train_norm['Cover_Type'] == 6)]\n",
    "\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "X , Y = df_train_norm_1_2.drop(columns=['Id','Cover_Type']).to_numpy(), df_train_norm_1_2.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "predict_fn = lambda x: etc.predict_proba(x).astype(float)\n",
    "df_train_exp=df_train_norm_1_2.drop(columns=['Id','Cover_Type'])\n",
    "# Create Lime Explainer\n",
    "explainer =  lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = list(df_train_exp.columns), class_names = ['3','6'])\n",
    "\n",
    "ones = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 3]\n",
    "k=0\n",
    "for i in range(len(ones)):\n",
    "    exp = explainer.explain_instance(X_val[ones[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[ones[i]]])[0])\n",
    "    if probs.argmax() != 1:\n",
    "        continue\n",
    "    print(probs)\n",
    "    k+=1\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    if k == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df_one = df_train_norm[df_train_norm['Cover_Type'] == 3]\n",
    "\n",
    "sn.histplot(data=df_one, x='sl_asp')\n",
    "\n",
    "plt.figure()\n",
    "df_two = df_train_norm[df_train_norm['Cover_Type'] == 6]\n",
    "sn.histplot(data=df_two, x='sl_asp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "# if not installed ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    lgbz = lgb.LGBMClassifier(n_estimators=100, boosting_type='gbdt', max_depth=8, num_leaves=90)\n",
    "    df_submission = submit(model=lgbz,version=i)\n",
    "    df_submission.to_csv(\"submit_\" + str(i) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperparameters: boosting_Type: 'gbdt', n_estimators = 100, max_depth = 8, num_leaves = 90\")\n",
    "print(\"With the best hyperparameters, extra tree model achieves: \" + \" 0.8806 accuracy in 2.85 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    lgbz = lgb.LGBMClassifier(n_estimators=100, boosting_type='gbdt', max_depth=8, num_leaves=90)\n",
    "    df_train_norm = pipeline(df_train, version=i)\n",
    "    score_model(lgbz, df_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Validation/Sanity Checks (LIME) <a id =\"100\"></a>\n",
    "\n",
    "Local interpretable model-agnostic explanations (*LIME*) is a Python library for model explainability. \n",
    "\n",
    "In our project, we used *LIME* to understand:\n",
    "- Most important features between cover types\n",
    "- Misclassifications\n",
    "- Strength of newly engineered features\n",
    "\n",
    "Overall, we see that Elevation, different distance metrics (Vert/Horiz Distance to Hydrology, Horiz Distance to Fire Points), and Aspect were important in influencing classfications between 1/2 and 3/6. For feature engineering, we focused our energy on on transforming these features. This would be the biggest distinction between version=0 and version=3 of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selecting K Best Features\n",
    "\n",
    "We will use sklearn's SelectKBest in its feature selection library. We can isolate cover type buckets that are most problematic and often confused by our model. This would most often be 1/2 and 3/6. \n",
    "\n",
    "First, bucketing all Soil Types and Wilderness Areas in single features could simplify the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soils(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    soils = ['Soil_Type' + str(i) for i in range(1,41) if i != 7]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Soil_Type1')\n",
    "    fort_ind = list(df_train_norm_copy.columns).index('Soil_Type40')\n",
    "    df_train_norm_copy['Soil_Type'] = (df_train_norm_copy.iloc[:, one_ind:fort_ind] == 1).idxmax(1).str.replace('Soil_Type','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=soils, inplace=True)\n",
    "    return df_train_norm_copy\n",
    "\n",
    "def wa(model):\n",
    "    df_train_norm_copy = df_train_norm.copy()\n",
    "    was = ['Wilderness_Area' + str(i) for i in range(1,5)]\n",
    "    one_ind = list(df_train_norm_copy.columns).index('Wilderness_Area1')\n",
    "    four_ind = list(df_train_norm_copy.columns).index('Wilderness_Area4')\n",
    "    df_train_norm_copy['Wilderness_Area'] = (df_train_norm_copy.iloc[:, one_ind:four_ind] == 1).idxmax(1).str.replace('Wilderness_Area','').astype(float)\n",
    "    df_train_norm_copy.drop(columns=was, inplace=True)\n",
    "    return df_train_norm_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 CT 1&2 (version=0)\n",
    "\n",
    "Most important features here are Elevation, Soil_Type, Hillshades, Horizontal Distance_To_Fire_Points, and Aspect. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 2) | (df_train_norm['Cover_Type'] == 1)]\n",
    "X , Y = df_train_sp.drop(columns=['Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "df_train_sp = df_train_sp.drop(columns=['Cover_Type'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j+1) + \". \" + df_train_sp.columns[ind_sc[i]] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 CT 1&2 (version=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 2) | (df_train_norm['Cover_Type'] == 1)]\n",
    "X , Y = df_train_sp.drop(columns=['Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "df_train_sp = df_train_sp.drop(columns=['Cover_Type'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j+1) + \". \" + df_train_sp.columns[ind_sc[i]] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 CT 3&6 (version=0)\n",
    "\n",
    "Most important features here are Soil_Type, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Hydrology, and Horizontal_Distance_To_Fire_Points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 3) | (df_train_norm['Cover_Type'] == 6)]\n",
    "X , Y = df_train_sp.drop(columns=['Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "df_train_sp = df_train_sp.drop(columns=['Cover_Type'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j+1) + \". \" + df_train_sp.columns[ind_sc[i]] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 CT 3&6 (version=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_norm = pipeline(df_train, version=3)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "\n",
    "df_train_sp = df_train_norm[(df_train_norm['Cover_Type'] == 3) | (df_train_norm['Cover_Type'] == 6)]\n",
    "X , Y = df_train_sp.drop(columns=['Cover_Type']).to_numpy(), df_train_sp.Cover_Type.to_numpy()\n",
    "df_train_sp = df_train_sp.drop(columns=['Cover_Type'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "X_new = SelectKBest(k='all').fit(X_train,y_train)\n",
    "scores = X_new.scores_\n",
    "ind_sc = np.argsort(scores)[::-1]\n",
    "j = 0\n",
    "for i in range(len(ind_sc)):\n",
    "    if np.isnan(scores[ind_sc[i]]):\n",
    "        continue\n",
    "    print(str(j+1) + \". \" + df_train_sp.columns[ind_sc[i]] + \": \" + str(scores[ind_sc[i]]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 LIME Explainer \n",
    "\n",
    "With LIME, we can see for CT1&2, the largest area of misclassifications is Elevation between 2700 and 3100 and Aspect between 127 and 250. Here, will we be showing 3 examples for Examples of CT1 where the model classes as CT2 and Examples of CT2 where the model classes as CT1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-75c633c4ac99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_train_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoils\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_train_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_train_norm_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cover_Type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_train_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cover_Type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_norm = pipeline(df_train, version=0)\n",
    "df_train_norm = soils(df_train_norm)\n",
    "df_train_norm = wa(df_train_norm)\n",
    "\n",
    "df_train_norm_1_2 = df_train_norm[(df_train_norm['Cover_Type'] == 1) | (df_train_norm['Cover_Type'] == 2)]\n",
    "\n",
    "etc = ExtraTreesClassifier(n_jobs=-1, random_state=0)\n",
    "X , Y = df_train_norm_1_2.drop(columns=['Cover_Type']).to_numpy(), df_train_norm_1_2.Cover_Type.to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=.33, random_state=0)\n",
    "etc.fit(X_train,y_train)\n",
    "predict_fn = lambda x: etc.predict_proba(x).astype(float)\n",
    "df_train_exp=df_train_norm.drop(columns=['Cover_Type'])\n",
    "# Create Lime Explainer\n",
    "explainer =  lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = list(df_train_exp.columns), class_names = ['1','2'])\n",
    "\n",
    "ones = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 1]\n",
    "twos = [ind for ind in range(y_val.shape[0]) if y_val[ind] == 2]\n",
    "\n",
    "\n",
    "k = 0\n",
    "examples_one = []\n",
    "examples_two = []\n",
    "\n",
    "print(\"\\n Examples of CT1 where the model classes as CT2: \\n\")\n",
    "for i in range(len(ones)):\n",
    "    exp = explainer.explain_instance(X_val[ones[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[ones[i]]])[0])\n",
    "    if probs.argmax() != 1:\n",
    "        continue\n",
    "    \n",
    "    k+=1\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    exp.save_to_file('lime_ct12_i.html')\n",
    "    examples_one.append(X_val[ones[i]])\n",
    "    if k == 3:\n",
    "        break\n",
    "\n",
    "print(\"\\n Examples of CT2 where the model classes as CT1: \\n\")\n",
    "j = 0\n",
    "for i in range(len(twos)):\n",
    "    exp = explainer.explain_instance(X_val[twos[i]], predict_fn, num_features=10)\n",
    "    probs = np.array(etc.predict_proba([X_val[twos[i]]])[0])\n",
    "    if probs.argmax() != 0:\n",
    "        continue\n",
    "    \n",
    "    j+=1\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    exp.save_to_file('lime_ct21_i.html')\n",
    "    examples_two.append(X_val[twos[i]])\n",
    "    if j == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
